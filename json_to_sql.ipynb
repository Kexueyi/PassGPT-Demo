{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import os\n",
    "import json\n",
    "\n",
    "password = os.getenv('MYSQL_ROOT_KEY')\n",
    "\n",
    "# 辅助函数，用于处理页面号码\n",
    "def extract_pages(pages):\n",
    "    if isinstance(pages, int):\n",
    "        pages = [pages]\n",
    "    return ', '.join(str(page) for page in pages)\n",
    "\n",
    "# 构建SQL插入语句的函数\n",
    "def build_sql_insert(table, columns, values):\n",
    "    formatted_values = []\n",
    "    for value in values:\n",
    "        if isinstance(value, str):\n",
    "            if value.startswith(\"(\") and value.endswith(\")\"):\n",
    "                # 对于子查询，直接添加，不需要引号\n",
    "                formatted_values.append(value)\n",
    "            else:\n",
    "                # 对于普通字符串，包括日期时间，确保替换内部单引号并用一对单引号包围\n",
    "                formatted_values.append(\"'\" + value.replace(\"'\", \"''\") + \"'\")\n",
    "        else:\n",
    "            # 对于整数或其他非字符串值，直接转换为字符串\n",
    "            formatted_values.append(str(value))\n",
    "    \n",
    "    # 构建并返回最终的SQL插入语句\n",
    "    return f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(formatted_values)});\"\n",
    "\n",
    "\n",
    "# 运行SQL命令的函数，执行并返回结果\n",
    "def run_sql_commands(connection, commands):\n",
    "    results = []\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            for command in commands:\n",
    "                cursor.execute(command)\n",
    "                if command.strip().upper().startswith('SELECT'):\n",
    "                    results.append(cursor.fetchall())\n",
    "        connection.commit()\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in transaction: {e}\")\n",
    "    return results\n",
    "\n",
    "# 加载JSON数据的函数\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load data from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 创建数据库连接\n",
    "connection = pymysql.connect(host='localhost', user='root', password=password, db='test', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据\n",
    "data = load_data('data/json/gpt.json')\n",
    "if data is None:\n",
    "    exit(1)\n",
    "\n",
    "sql_commands = []\n",
    "existing_courses = set()  # 用于跟踪已经处理过的课程代码\n",
    "\n",
    "# 查询现有的课程代码，以避免重复插入\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT course_code FROM courses\")\n",
    "        existing_courses = set(course['course_code'] for course in cursor.fetchall())\n",
    "except pymysql.MySQLError as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "\n",
    "# 插入课程数据\n",
    "for file_title, details in data.items():\n",
    "    # 提取课程代码，假设格式为 'EE6405_W1_Introduction_to_NLP'\n",
    "    course_code = file_title.split('_')[0]\n",
    "    if course_code not in existing_courses:  # 检查是否已存在\n",
    "        course_name = \"Natural Language Processing\"  # 假设所有课程名称相同，或者可以从某处提取\n",
    "        sql_commands.append(build_sql_insert('courses', ['course_code', 'course_name'], [course_code, course_name]))\n",
    "        existing_courses.add(course_code)  # 添加到已存在课程集合中，防止重复处理\n",
    "\n",
    "# 执行SQL命令\n",
    "run_sql_commands(connection, sql_commands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass-gpt\n",
      "code/Week 1.ipynb\n",
      "code/Week 10.ipynb\n",
      "code/Week 11.ipynb\n",
      "code/Week 2.ipynb\n",
      "code/Week 3.ipynb\n",
      "code/Week 4.ipynb\n",
      "code/Week 5.ipynb\n",
      "code/Week 6.ipynb\n",
      "code/Week 7.ipynb\n",
      "code/Week 8.ipynb\n",
      "code/Week 9.ipynb\n",
      "slides/EE6405_W10_ A survey of NLP applications across diverse industries_For Students.pdf\n",
      "slides/EE6405_W11_ Deep-dive into NLP_For Students.pdf\n",
      "slides/EE6405_W1_Introduction to NLP_For Students.pdf\n",
      "slides/EE6405_W2_Linguistic Analysis and Information Extraction_For Students.pdf\n",
      "slides/EE6405_W3_Term Weighting Scheme and Topic Modelling_For Students.pdf\n",
      "slides/EE6405_W4_Traditional ML and NLP Applications_For Students.pdf\n",
      "slides/EE6405_W5_EMaWE_For Students.pdf\n",
      "slides/EE6405_W6_NM.pdf\n",
      "slides/EE6405_W7_Transformer.pdf\n",
      "slides/EE6405_W8_HPT_For Students.pdf\n",
      "slides/EE6405_W9_TLLMs_For Students.pdf\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# 初始化S3客户端\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)\n",
    "\n",
    "for files in bucket.objects.all():\n",
    "    print(files.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slides Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "sql_commands = []\n",
    "\n",
    "for file_title, details in data.items():\n",
    "    course_code = file_title.split('_')[0]\n",
    "    file_name = file_title\n",
    "    title = details[\"Title\"]\n",
    "    material_type = details['Material Type']\n",
    "    file_path = f\"slides/{file_title}\"  \n",
    "    teaching_week = int(details['Teaching Week'].replace('week ', '')) # 从 \"week 1\" 提取 \"1\"\n",
    "    creation_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') \n",
    "\n",
    "    # 构建插入到files表的SQL命令\n",
    "    insert_command = build_sql_insert(\n",
    "        'files', \n",
    "        ['course_id', 'file_name', 'title', 'file_type', 'file_path', 'teaching_week', 'creation_date'], \n",
    "        [\n",
    "            f\"(SELECT course_id FROM courses WHERE course_code='{course_code}')\",  # 从courses表中获取course_id\n",
    "            file_name, \n",
    "            title,\n",
    "            material_type,\n",
    "            file_path,  \n",
    "            teaching_week,  # 整数值转换为字符串，但不加引号\n",
    "            creation_date\n",
    "        ]\n",
    "    )\n",
    "    sql_commands.append(insert_command)\n",
    "\n",
    "# 执行SQL命令\n",
    "run_sql_commands(connection, sql_commands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化概念插入的SQL命令列表\n",
    "sql_commands_concepts = []\n",
    "concept_to_subconcepts = []  # 存储概念与其子概念的关系，用于延后处理\n",
    "\n",
    "for file_title, details in data.items():\n",
    "    file_name = file_title\n",
    "    concepts = details.get('Concepts', [])  # 获取概念列表\n",
    "    if concepts:  # 如果存在概念\n",
    "        for concept in concepts:\n",
    "            concept_name = concept['name']\n",
    "            concept_pages = extract_pages(concept.get('Page', ''))\n",
    "            # 插入概念\n",
    "            insert_concept = build_sql_insert(\n",
    "                'concepts',\n",
    "                ['file_id', 'concept_name', 'concept_page'],\n",
    "                [f\"(SELECT file_id FROM files WHERE file_name='{file_name}')\", concept_name, concept_pages]\n",
    "            )\n",
    "            sql_commands_concepts.append(insert_concept)\n",
    "            \n",
    "            # 暂存子概念信息，稍后处理\n",
    "            subconcepts = concept.get('Subconcepts', [])\n",
    "            if subconcepts:\n",
    "                for subconcept in subconcepts:\n",
    "                    subconcept_name = subconcept['name']\n",
    "                    subconcept_pages = extract_pages(subconcept.get('Page', ''))\n",
    "                    concept_to_subconcepts.append((file_name, concept_name, subconcept_name, subconcept_pages))\n",
    "\n",
    "# 执行概念的插入\n",
    "run_sql_commands(connection, sql_commands_concepts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('EE6405_W1_Introduction_to_NLP', 'Overview of NLP'): (1, 1), ('EE6405_W1_Introduction_to_NLP', 'Historical Background'): (1, 2), ('EE6405_W1_Introduction_to_NLP', 'Approaches to NLP'): (1, 3), ('EE6405_W1_Introduction_to_NLP', 'Preprocessing Techniques'): (1, 4), ('EE6405_W2_Linguistic Analysis and Information Extraction', 'Introduction to Information Extraction'): (2, 5), ('EE6405_W2_Linguistic Analysis and Information Extraction', 'Named Entity Recognition (NER)'): (2, 6), ('EE6405_W2_Linguistic Analysis and Information Extraction', 'Part-Of-Speech Tagging'): (2, 7), ('EE6405_W2_Linguistic Analysis and Information Extraction', 'Dependency Parsing'): (2, 8), ('EE6405_W3_Term Weighting Scheme and Topic Modelling_For Students', 'Term Weighting Schemes'): (3, 9), ('EE6405_W3_Term Weighting Scheme and Topic Modelling_For Students', 'Topic Modeling'): (3, 10), ('EE6405_W3_Term Weighting Scheme and Topic Modelling_For Students', 'Dimensionality Reduction'): (3, 11), ('EE6405_W4_Traditional ML and NLP Applications', 'Text Classification'): (4, 12), ('EE6405_W4_Traditional ML and NLP Applications', 'Clustering'): (4, 13), ('EE6405_W5_EMaWE', 'Evaluation Metrics'): (5, 14), ('EE6405_W5_EMaWE', 'Word Embeddings'): (5, 15), ('EE6405_W6_Neural Language Models', 'Sequential Data'): (6, 16), ('EE6405_W6_Neural Language Models', 'RNNs'): (6, 17), ('EE6405_W6_Neural Language Models', 'LSTMs'): (6, 18), ('EE6405_W6_Neural Language Models', 'GRUs'): (6, 19), ('EE6405_W7_Transformers', 'Seq2Seq Models'): (7, 21), ('EE6405_W7_Transformers', 'Attention Mechanism'): (7, 22), ('EE6405_W7_Transformers', 'Transformer Models'): (7, 23), ('EE6405_W8_HyperParameter Tuning', 'K-Fold Cross Validation'): (8, 24), ('EE6405_W8_HyperParameter Tuning', 'Optimisers'): (8, 25), ('EE6405_W8_HyperParameter Tuning', 'Loss Functions'): (8, 26), ('EE6405_W9_Transformer Based LLMs', 'Transformer Recap'): (9, 31), ('EE6405_W9_Transformer Based LLMs', 'Pre Training Objectives'): (9, 32), ('EE6405_W9_Transformer Based LLMs', 'Tokenizers'): (9, 36), ('EE6405_W9_Transformer Based LLMs', 'BERT Variants'): (9, 39), ('EE6405_W9_Transformer Based LLMs', 'GPT Series'): (9, 40), ('EE6405_W10_NLP_Applications_Across_Industries', 'Overview'): (10, 43), ('EE6405_W10_NLP_Applications_Across_Industries', 'Healthcare'): (10, 44), ('EE6405_W10_NLP_Applications_Across_Industries', 'Finance and Banking'): (10, 45), ('EE6405_W10_NLP_Applications_Across_Industries', 'Retail and E-Commerce'): (10, 46), ('EE6405_W10_NLP_Applications_Across_Industries', 'Legal Industry'): (10, 47), ('EE6405_W10_NLP_Applications_Across_Industries', 'Automotive Industry'): (10, 48), ('EE6405_W10_NLP_Applications_Across_Industries', 'Publishing Industry'): (10, 49), ('EE6405_W10_NLP_Applications_Across_Industries', 'Education'): (10, 50), ('EE6405_W10_NLP_Applications_Across_Industries', 'Travel and Hospitality'): (10, 51), ('EE6405_W10_NLP_Applications_Across_Industries', 'Media and Entertainment'): (10, 52), ('EE6405_W10_NLP_Applications_Across_Industries', 'Government & Public Sector'): (10, 53), ('EE6405_W11_Deep_dive_into_NLP', 'Fraud Detection in E-commerce'): (11, 54), ('EE6405_W11_Deep_dive_into_NLP', 'Human Trafficking Detection'): (11, 55), ('EE6405_W11_Deep_dive_into_NLP', 'ChatGPT-based Applications'): (11, 56)}\n"
     ]
    }
   ],
   "source": [
    "# 查询并缓存所有的 concept_id 和对应的 file_id\n",
    "concept_ids = {}\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        for file_name, concept_name, _, _ in concept_to_subconcepts:\n",
    "            # 确保我们只查询一次每个概念\n",
    "            if (file_name, concept_name) not in concept_ids:\n",
    "                query = f\"SELECT concept_id, (SELECT file_id FROM files WHERE file_name='{file_name}') as file_id FROM concepts WHERE concept_name='{concept_name}' AND file_id=(SELECT file_id FROM files WHERE file_name='{file_name}')\"\n",
    "                cursor.execute(query)\n",
    "                result = cursor.fetchone()\n",
    "                if result:\n",
    "                    concept_ids[(file_name, concept_name)] = (result['file_id'], result['concept_id'])\n",
    "except pymysql.MySQLError as e:\n",
    "    print(f\"Database error during fetching ids: {e}\")   \n",
    "\n",
    "print(concept_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用缓存的ID构建子概念的插入命令\n",
    "sql_commands_subconcepts = []\n",
    "for file_name, parent_concept_name, subconcept_name, subconcept_pages in concept_to_subconcepts:\n",
    "    if (file_name, parent_concept_name) in concept_ids:\n",
    "        file_id, parent_id = concept_ids[(file_name, parent_concept_name)]\n",
    "        insert_subconcept = build_sql_insert(\n",
    "            'concepts',\n",
    "            ['parent_id', 'file_id', 'concept_name', 'concept_page'],\n",
    "            [str(parent_id), str(file_id), subconcept_name, subconcept_pages]\n",
    "        )\n",
    "        sql_commands_subconcepts.append(insert_subconcept)\n",
    "\n",
    "# 执行子概念的插入\n",
    "run_sql_commands(connection, sql_commands_subconcepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODO: 两个章节有summary, 但是很混乱。\n",
    "# for summary in details.get('Summary', []):\n",
    "#     summary_page = ', '.join(map(str, summary.get('page', summary.get('Page', []))))\n",
    "#     key_points = ', '.join(summary.get('keyPoints', ''))\n",
    "#     sql_commands.append(build_sql_insert('summaries', ['file_id', 'summary_page', 'key_points'], [f\"(SELECT file_id FROM files WHERE file_name='{file_name}')\", f\"'{summary_page}'\", f\"'{key_points}'\"]))\n",
    "\n",
    "#TODO: 公式有时候是个列表（line 324)，有时候直接变成了介绍后面跟着页面。\n",
    "# for example_type, examples in [('formula', subconcept.get('Formula', [])), ('code', subconcept.get('Code', []))]:\n",
    "#     for example in examples:\n",
    "#         example_description = example['description']\n",
    "#         example_pages = extract_pages(example.get('Page', []))\n",
    "\n",
    "#         sql_commands.append(build_sql_insert('examples', ['concept_id', 'example_name', 'example_page', 'example_description', 'example_type'], [f\"(SELECT concept_id FROM concepts WHERE concept_name='{subconcept_name}')\", f\"'{example_description}'\", f\"'{example_pages}'\", f\"'{example_description}'\", f\"'{example_type}'\"]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
