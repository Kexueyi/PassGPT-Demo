[
  {
    "type": "Import",
    "start": {
      "line": 11,
      "column": 0
    },
    "end": {
      "line": 11,
      "column": 19
    },
    "name": "pandas",
    "alias": "pd"
  },
  {
    "type": "Import",
    "start": {
      "line": 12,
      "column": 0
    },
    "end": {
      "line": 12,
      "column": 9
    },
    "name": "re",
    "alias": null
  },
  {
    "type": "Import",
    "start": {
      "line": 13,
      "column": 0
    },
    "end": {
      "line": 13,
      "column": 18
    },
    "name": "numpy",
    "alias": "np"
  },
  {
    "type": "Import",
    "start": {
      "line": 14,
      "column": 0
    },
    "end": {
      "line": 14,
      "column": 23
    },
    "name": "tensorflow",
    "alias": "tf"
  },
  {
    "type": "Import",
    "start": {
      "line": 15,
      "column": 0
    },
    "end": {
      "line": 15,
      "column": 11
    },
    "name": "nltk",
    "alias": null
  },
  {
    "type": "Import",
    "from": "nltk.corpus",
    "import": {
      "name": "stopwords",
      "alias": null
    },
    "start": {
      "line": 16,
      "column": 0
    },
    "end": {
      "line": 16,
      "column": 33
    }
  },
  {
    "type": "Import",
    "from": "nltk.tokenize",
    "import": {
      "name": "word_tokenize",
      "alias": null
    },
    "start": {
      "line": 17,
      "column": 0
    },
    "end": {
      "line": 17,
      "column": 39
    }
  },
  {
    "type": "Assignment",
    "body": "imdb_df=pd.read_csv(r\"C:\\Users\\liuru\\Desktop\\EE6405\\Data\\IMDB\\IMDB Dataset.csv\")",
    "start": {
      "line": 27,
      "column": 0
    },
    "end": {
      "line": 27,
      "column": 82
    }
  },
  {
    "type": "Assignment",
    "body": "df_positive=imdb_df[imdb_df['sentiment']=='positive'][:5000]",
    "start": {
      "line": 28,
      "column": 0
    },
    "end": {
      "line": 28,
      "column": 62
    }
  },
  {
    "type": "Assignment",
    "body": "df_negative=imdb_df[imdb_df['sentiment']=='negative'][:5000]",
    "start": {
      "line": 29,
      "column": 0
    },
    "end": {
      "line": 29,
      "column": 62
    }
  },
  {
    "type": "Assignment",
    "body": "imdb=pd.concat([df_positive,df_negative])",
    "start": {
      "line": 30,
      "column": 0
    },
    "end": {
      "line": 30,
      "column": 44
    }
  },
  null,
  {
    "type": "Assignment",
    "body": "imdb['review']=imdb['review'].apply(lambdax:re.sub('(<.*?>)',' ',x))",
    "start": {
      "line": 38,
      "column": 0
    },
    "end": {
      "line": 38,
      "column": 74
    }
  },
  {
    "type": "Assignment",
    "body": "imdb['review']=imdb['review'].apply(lambdax:re.sub('[,\\.!?:()\"]','',x))",
    "start": {
      "line": 40,
      "column": 0
    },
    "end": {
      "line": 40,
      "column": 77
    }
  },
  {
    "type": "Assignment",
    "body": "imdb['review']=imdb['review'].apply(lambdax:x.strip())",
    "start": {
      "line": 42,
      "column": 0
    },
    "end": {
      "line": 42,
      "column": 58
    }
  },
  {
    "type": "Assignment",
    "body": "imdb['review']=imdb['review'].apply(lambdax:re.sub('[^a-zA-Z\"]',' ',x))",
    "start": {
      "line": 44,
      "column": 0
    },
    "end": {
      "line": 44,
      "column": 75
    }
  },
  {
    "type": "Assignment",
    "body": "imdb['review']=imdb['review'].apply(lambdax:x.lower())",
    "start": {
      "line": 46,
      "column": 0
    },
    "end": {
      "line": 46,
      "column": 58
    }
  },
  {
    "type": "Function call",
    "name": "nltk.download",
    "arguments": "('wordnet')",
    "start": {
      "line": 52,
      "column": 0
    },
    "end": {
      "line": 52,
      "column": 24
    }
  },
  {
    "type": "Import",
    "from": "nltk.stem",
    "import": {
      "name": "WordNetLemmatizer",
      "alias": null
    },
    "start": {
      "line": 53,
      "column": 0
    },
    "end": {
      "line": 53,
      "column": 39
    }
  },
  {
    "type": "Import",
    "from": "nltk.tokenize",
    "import": {
      "name": "word_tokenize",
      "alias": null
    },
    "start": {
      "line": 54,
      "column": 0
    },
    "end": {
      "line": 54,
      "column": 39
    }
  },
  {
    "type": "Function call",
    "name": "nltk.download",
    "arguments": "('averaged_perceptron_tagger')",
    "start": {
      "line": 55,
      "column": 0
    },
    "end": {
      "line": 55,
      "column": 43
    }
  },
  {
    "type": "Assignment",
    "body": "lemmatizer=WordNetLemmatizer()",
    "start": {
      "line": 56,
      "column": 0
    },
    "end": {
      "line": 56,
      "column": 31
    }
  },
  {
    "type": "Import",
    "from": "nltk.corpus",
    "import": {
      "name": "wordnet",
      "alias": null
    },
    "start": {
      "line": 57,
      "column": 0
    },
    "end": {
      "line": 57,
      "column": 31
    }
  },
  {
    "type": "Function declaration",
    "body": "defpos_tagger(nltk_tag):     ifnltk_tag.startswith('J'):         returnwordnet.ADJ  elifnltk_tag.startswith('V'):         returnwordnet.VERB  elifnltk_tag.startswith('N'):         returnwordnet.NOUN  elifnltk_tag.startswith('R'):         returnwordnet.ADV  else:         returnNone\n\n\n",
    "start": {
      "line": 63,
      "column": 0
    },
    "end": {
      "line": 74,
      "column": 1
    }
  },
  {
    "type": "Function declaration",
    "body": "deftagged_lemma(string):     pos_tagged=nltk.pos_tag(nltk.word_tokenize(string)) wordnet_tagged=list(map(lambdax:(x[0],pos_tagger(x[1])),pos_tagged)) lemmatized_sentence=[] forword,taginwordnet_tagged:         iftagisNone:             lemmatized_sentence.append(word)  else:             lemmatized_sentence.append(lemmatizer.lemmatize(word,tag))   lemmatized_sentence=\" \".join(lemmatized_sentence) returnlemmatized_sentence\n\n",
    "start": {
      "line": 74,
      "column": 0
    },
    "end": {
      "line": 94,
      "column": 1
    }
  },
  {
    "type": "Assignment",
    "body": "imdb['review']=imdb['review'].apply(tagged_lemma)",
    "start": {
      "line": 94,
      "column": 0
    },
    "end": {
      "line": 94,
      "column": 49
    }
  },
  {
    "type": "Assignment",
    "body": "imdb.sentiment=imdb.sentiment.apply(lambdax:1ifx=='positive'else0)",
    "start": {
      "line": 100,
      "column": 0
    },
    "end": {
      "line": 100,
      "column": 74
    }
  },
  {
    "type": "Import",
    "from": "sklearn.model_selection",
    "import": {
      "name": "train_test_split",
      "alias": null
    },
    "start": {
      "line": 106,
      "column": 0
    },
    "end": {
      "line": 106,
      "column": 52
    }
  },
  {
    "type": "Assignment",
    "body": "training_set,test_set=train_test_split(imdb,test_size=0.25,random_state=42)",
    "start": {
      "line": 107,
      "column": 0
    },
    "end": {
      "line": 107,
      "column": 79
    }
  },
  null,
  {
    "type": "Import",
    "start": {
      "line": 124,
      "column": 0
    },
    "end": {
      "line": 124,
      "column": 18
    },
    "name": "numpy",
    "alias": "np"
  },
  {
    "type": "Import",
    "start": {
      "line": 125,
      "column": 0
    },
    "end": {
      "line": 125,
      "column": 23
    },
    "name": "tensorflow",
    "alias": "tf"
  },
  {
    "type": "Assignment",
    "body": "reviews=training_set['review'].tolist()",
    "start": {
      "line": 134,
      "column": 0
    },
    "end": {
      "line": 134,
      "column": 41
    }
  },
  {
    "type": "Assignment",
    "body": "VOCAB_SIZE=1000",
    "start": {
      "line": 140,
      "column": 0
    },
    "end": {
      "line": 140,
      "column": 17
    }
  },
  {
    "type": "Assignment",
    "body": "encoder=tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE,output_mode='int',pad_to_max_tokens=True)",
    "start": {
      "line": 141,
      "column": 0
    },
    "end": {
      "line": 145,
      "column": 1
    }
  },
  {
    "type": "Function call",
    "name": "encoder.adapt",
    "arguments": "(reviews)",
    "start": {
      "line": 146,
      "column": 0
    },
    "end": {
      "line": 146,
      "column": 22
    }
  },
  {
    "type": "Assignment",
    "body": "vocab=np.array(encoder.get_vocabulary())",
    "start": {
      "line": 148,
      "column": 0
    },
    "end": {
      "line": 148,
      "column": 42
    }
  },
  null,
  null,
  {
    "type": "Assignment",
    "body": "encoded_example=encoder(training_set.review.iloc[3]).numpy()",
    "start": {
      "line": 166,
      "column": 0
    },
    "end": {
      "line": 166,
      "column": 62
    }
  },
  null,
  {
    "type": "Assignment",
    "body": "simpleRNN=tf.keras.Sequential([encoder,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True),tf.keras.layers.SimpleRNN(64),tf.keras.layers.Dense(64,activation='relu'),tf.keras.layers.Dense(1)])",
    "start": {
      "line": 186,
      "column": 0
    },
    "end": {
      "line": 196,
      "column": 2
    }
  },
  {
    "type": "Function call",
    "name": "simpleRNN.compile",
    "arguments": "(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])",
    "start": {
      "line": 204,
      "column": 0
    },
    "end": {
      "line": 206,
      "column": 35
    }
  },
  {
    "type": "Assignment",
    "body": "historyRNN=simpleRNN.fit(training_set['review'],training_set['sentiment'],epochs=10,validation_data=(test_set['review'],test_set['sentiment']),validation_steps=30)",
    "start": {
      "line": 212,
      "column": 0
    },
    "end": {
      "line": 214,
      "column": 40
    }
  },
  {
    "type": "Assignment",
    "body": "lstm=tf.keras.Sequential([encoder,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True),tf.keras.layers.LSTM(64),tf.keras.layers.Dense(64,activation='relu'),tf.keras.layers.Dense(1)])",
    "start": {
      "line": 223,
      "column": 0
    },
    "end": {
      "line": 233,
      "column": 2
    }
  },
  {
    "type": "Function call",
    "name": "lstm.compile",
    "arguments": "(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])",
    "start": {
      "line": 239,
      "column": 0
    },
    "end": {
      "line": 241,
      "column": 35
    }
  },
  {
    "type": "Assignment",
    "body": "historyLSTM=lstm.fit(training_set['review'],training_set['sentiment'],epochs=10,validation_data=(test_set['review'],test_set['sentiment']),validation_steps=30)",
    "start": {
      "line": 247,
      "column": 0
    },
    "end": {
      "line": 249,
      "column": 40
    }
  },
  {
    "type": "Assignment",
    "body": "biDir=tf.keras.Sequential([encoder,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True),tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),tf.keras.layers.Dense(64,activation='relu'),tf.keras.layers.Dense(1)])",
    "start": {
      "line": 258,
      "column": 0
    },
    "end": {
      "line": 268,
      "column": 2
    }
  },
  {
    "type": "Function call",
    "name": "biDir.compile",
    "arguments": "(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])",
    "start": {
      "line": 274,
      "column": 0
    },
    "end": {
      "line": 276,
      "column": 35
    }
  },
  {
    "type": "Assignment",
    "body": "historybiDir=biDir.fit(training_set['review'],training_set['sentiment'],epochs=10,validation_data=(test_set['review'],test_set['sentiment']),validation_steps=30)",
    "start": {
      "line": 282,
      "column": 0
    },
    "end": {
      "line": 284,
      "column": 40
    }
  },
  {
    "type": "Assignment",
    "body": "gru=tf.keras.Sequential([encoder,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True),tf.keras.layers.GRU(64),tf.keras.layers.Dense(64,activation='relu'),tf.keras.layers.Dense(1)])",
    "start": {
      "line": 293,
      "column": 0
    },
    "end": {
      "line": 303,
      "column": 2
    }
  },
  {
    "type": "Function call",
    "name": "gru.compile",
    "arguments": "(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])",
    "start": {
      "line": 309,
      "column": 0
    },
    "end": {
      "line": 311,
      "column": 35
    }
  },
  {
    "type": "Assignment",
    "body": "historyGRU=gru.fit(training_set['review'],training_set['sentiment'],epochs=10,validation_data=(test_set['review'],test_set['sentiment']),validation_steps=30)",
    "start": {
      "line": 317,
      "column": 0
    },
    "end": {
      "line": 319,
      "column": 40
    }
  },
  {
    "type": "Import",
    "start": {
      "line": 327,
      "column": 0
    },
    "end": {
      "line": 327,
      "column": 31
    },
    "name": "matplotlib.pyplot",
    "alias": "plt"
  },
  {
    "type": "Function declaration",
    "body": "defplot_graphs(history,metric):   plt.plot(history.history[metric]) plt.plot(history.history['val_'+metric],'') plt.xlabel(\"Epochs\") plt.ylabel(metric) plt.legend([metric,'val_'+metric])\n\n",
    "start": {
      "line": 330,
      "column": 0
    },
    "end": {
      "line": 341,
      "column": 1
    }
  },
  {
    "type": "Function call",
    "name": "plot_graphs",
    "arguments": "(historyRNN,\"accuracy\")",
    "start": {
      "line": 341,
      "column": 0
    },
    "end": {
      "line": 341,
      "column": 35
    }
  },
  {
    "type": "Function call",
    "name": "plot_graphs",
    "arguments": "(historyLSTM,\"accuracy\")",
    "start": {
      "line": 347,
      "column": 0
    },
    "end": {
      "line": 347,
      "column": 36
    }
  },
  {
    "type": "Function call",
    "name": "plot_graphs",
    "arguments": "(historybiDir,\"accuracy\")",
    "start": {
      "line": 353,
      "column": 0
    },
    "end": {
      "line": 353,
      "column": 37
    }
  },
  {
    "type": "Function call",
    "name": "plot_graphs",
    "arguments": "(historyGRU,\"accuracy\")",
    "start": {
      "line": 359,
      "column": 0
    },
    "end": {
      "line": 359,
      "column": 35
    }
  }
]