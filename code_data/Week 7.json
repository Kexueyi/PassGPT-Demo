[
  {
    "type": "Import",
    "start": {
      "line": 7,
      "column": 0
    },
    "end": {
      "line": 7,
      "column": 12
    },
    "name": "torch",
    "alias": null
  },
  {
    "type": "Import",
    "start": {
      "line": 8,
      "column": 0
    },
    "end": {
      "line": 8,
      "column": 21
    },
    "name": "torch.nn",
    "alias": "nn"
  },
  {
    "type": "Import",
    "start": {
      "line": 9,
      "column": 0
    },
    "end": {
      "line": 9,
      "column": 27
    },
    "name": "torch.optim",
    "alias": "optim"
  },
  {
    "type": "Import",
    "start": {
      "line": 10,
      "column": 0
    },
    "end": {
      "line": 10,
      "column": 31
    },
    "name": "torch.utils.data",
    "alias": "data"
  },
  {
    "type": "Import",
    "start": {
      "line": 11,
      "column": 0
    },
    "end": {
      "line": 11,
      "column": 11
    },
    "name": "math",
    "alias": null
  },
  {
    "type": "Import",
    "start": {
      "line": 12,
      "column": 0
    },
    "end": {
      "line": 12,
      "column": 11
    },
    "name": "copy",
    "alias": null
  },
  {
    "type": "Class declaration",
    "body": "classMultiHeadAttention(nn.Module):     \"\"\"\n    The init constructor checks whether the provided d_model is divisible by the number of heads (num_heads). \n    It sets up the necessary parameters and creates linear transformations for\n    query(W_q), key(W_k) and output(W_o) projections\n    \"\"\" def__init__(self,d_model,num_heads):         super(MultiHeadAttention,self).__init__() assertd_model%num_heads==0,\"d_model must be divisible by num_heads\" self.d_model=d_model self.num_heads=num_heads self.d_k=d_model//num_heads self.W_q=nn.Linear(d_model,d_model) self.W_k=nn.Linear(d_model,d_model) self.W_v=nn.Linear(d_model,d_model) self.W_o=nn.Linear(d_model,d_model)  \"\"\"\n     The scaled_dot_product_attention function computes the scaled dot-product attention given the \n     query (Q), key (K), and value (V) matrices. It uses the scaled dot product formula, applies a mask if \n     provided, and computes the attention probabilities using the softmax function.\n    \"\"\" defscaled_dot_product_attention(self,Q,K,V,mask=None):         attn_scores=torch.matmul(Q,K.transpose(-2,-1))/math.sqrt(self.d_k) ifmaskisnotNone:             attn_scores=attn_scores.masked_fill(mask==0,-1e9)  attn_probs=torch.softmax(attn_scores,dim=-1) output=torch.matmul(attn_probs,V) returnoutput  \"\"\"\n    The split_heads and combine_heads functions handle the splitting and combining of the attention heads.\n    They reshape the input tensor to allow parallel processing of different attention heads.\n    \"\"\" defsplit_heads(self,x):         batch_size,seq_length,d_model=x.size() returnx.view(batch_size,seq_length,self.num_heads,self.d_k).transpose(1,2)  defcombine_heads(self,x):         batch_size,_,seq_length,d_k=x.size() returnx.transpose(1,2).contiguous().view(batch_size,seq_length,self.d_model)  \"\"\"\n     The forward function takes input query (Q), key (K), and value (V) tensors, \n     applies linear transformations, splits them into multiple heads, performs scaled dot-product attention,\n     combines the attention heads, and applies a final linear transformation.\n    \"\"\" defforward(self,Q,K,V,mask=None):         Q=self.split_heads(self.W_q(Q)) K=self.split_heads(self.W_k(K)) V=self.split_heads(self.W_v(V)) attn_output=self.scaled_dot_product_attention(Q,K,V,mask) output=self.W_o(self.combine_heads(attn_output)) returnoutput\n\n\n",
    "start": {
      "line": 22,
      "column": 0
    },
    "end": {
      "line": 87,
      "column": 1
    }
  },
  {
    "type": "Class declaration",
    "body": "classPositionWiseFeedForward(nn.Module):     \"\"\"\n    PositionWiseFeedForward module. It takes d_model as the input dimension and d_ff \n    as the hidden layer dimension. \n    Two linear layers (fc1 and fc2) are defined with ReLU activation in between.\n    \"\"\" def__init__(self,d_model,d_ff):         super(PositionWiseFeedForward,self).__init__() self.fc1=nn.Linear(d_model,d_ff) self.fc2=nn.Linear(d_ff,d_model) self.relu=nn.ReLU()  \"\"\"\n    The forward function takes an input tensor x, applies the first linear transformation (fc1), \n    applies the ReLU activation, and then applies the second linear transformation (fc2). \n    The output is the result of the second linear transformation.\n    \"\"\" defforward(self,x):         returnself.fc2(self.relu(self.fc1(x)))\n\n\n",
    "start": {
      "line": 87,
      "column": 0
    },
    "end": {
      "line": 115,
      "column": 1
    }
  },
  {
    "type": "Class declaration",
    "body": "classPositionalEncoding(nn.Module):     \"\"\"\n    The constructor (__init__) initializes the PositionalEncoding module. \n    It takes d_model as the dimension of the model and max_seq_length as the maximum sequence length. \n    It computes the positional encoding matrix (pe) using sine and cosine functions.\n    \"\"\" def__init__(self,d_model,max_seq_length):         super(PositionalEncoding,self).__init__() pe=torch.zeros(max_seq_length,d_model) position=torch.arange(0,max_seq_length,dtype=torch.float).unsqueeze(1) div_term=torch.exp(torch.arange(0,d_model,2).float()*-(math.log(10000.0)/d_model)) pe[:,0::2]=torch.sin(position*div_term) pe[:,1::2]=torch.cos(position*div_term) self.register_buffer('pe',pe.unsqueeze(0))  \"\"\"\n    The forward function takes an input tensor x and adds the positional encoding to it. \n    The positional encoding is truncated to match the length of the input sequence (x.size(1)).\n    \"\"\" defforward(self,x):         returnx+self.pe[:,:x.size(1)]\n\n\n",
    "start": {
      "line": 115,
      "column": 0
    },
    "end": {
      "line": 148,
      "column": 1
    }
  },
  {
    "type": "Class declaration",
    "body": "classEncoderLayer(nn.Module):     \"\"\"\n    The constructor (__init__) initializes the EncoderLayer module. \n    It takes hyperparameters such as d_model (model dimension), num_heads (number of attention heads), \n    d_ff (dimension of the feedforward network), and dropout (dropout rate). \n    It creates instances of MultiHeadAttention, PositionWiseFeedForward, and nn.LayerNorm. \n    Dropout is also defined as a module.\n    \"\"\" def__init__(self,d_model,num_heads,d_ff,dropout):         super(EncoderLayer,self).__init__() self.self_attn=MultiHeadAttention(d_model,num_heads) self.feed_forward=PositionWiseFeedForward(d_model,d_ff) self.norm1=nn.LayerNorm(d_model) self.norm2=nn.LayerNorm(d_model) self.dropout=nn.Dropout(dropout)  \"\"\"\n    The forward function takes an input tensor x and a mask. \n    It applies the self-attention mechanism (self.self_attn), adds the residual connection \n    with layer normalization, applies the position-wise feedforward network (self.feed_forward),\n    and again adds the residual connection with layer normalization. \n    Dropout is applied at both the self-attention and feedforward stages.\n    The mask parameter is used to mask certain positions during the self-attention step, \n    typically to prevent attending to future positions in a sequence.\n    \"\"\" defforward(self,x,mask):         attn_output=self.self_attn(x,x,x,mask) x=self.norm1(x+self.dropout(attn_output)) ff_output=self.feed_forward(x) x=self.norm2(x+self.dropout(ff_output)) returnx\n\n\n",
    "start": {
      "line": 148,
      "column": 0
    },
    "end": {
      "line": 189,
      "column": 1
    }
  },
  {
    "type": "Class declaration",
    "body": "classDecoderLayer(nn.Module):     \"\"\"\n    The constructor (__init__) initializes the DecoderLayer module. \n    It takes hyperparameters such as d_model (model dimension), num_heads (number of attention heads), \n    d_ff (dimension of the feedforward network), and dropout (dropout rate). \n    It creates instances of MultiHeadAttention for both self-attention (self.self_attn) and cross-attention\n    (self.cross_attn), PositionWiseFeedForward, and nn.LayerNorm. Dropout is also defined as a module.\n    \"\"\" def__init__(self,d_model,num_heads,d_ff,dropout):         super(DecoderLayer,self).__init__() self.self_attn=MultiHeadAttention(d_model,num_heads) self.cross_attn=MultiHeadAttention(d_model,num_heads) self.feed_forward=PositionWiseFeedForward(d_model,d_ff) self.norm1=nn.LayerNorm(d_model) self.norm2=nn.LayerNorm(d_model) self.norm3=nn.LayerNorm(d_model) self.dropout=nn.Dropout(dropout)  \"\"\"\n    The forward function takes an input tensor x, the output from the encoder (enc_output), \n    and masks for the source (src_mask) and target (tgt_mask). It applies the self-attention mechanism, \n    adds the residual connection with layer normalization, applies the cross-attention mechanism with the \n    encoder's output, adds another residual connection with layer normalization, applies the position-wise \n    feedforward network, and adds a final residual connection with layer normalization. \n    Dropout is applied at each stage.\n    \"\"\" defforward(self,x,enc_output,src_mask,tgt_mask):         attn_output=self.self_attn(x,x,x,tgt_mask) x=self.norm1(x+self.dropout(attn_output)) attn_output=self.cross_attn(x,enc_output,enc_output,src_mask) x=self.norm2(x+self.dropout(attn_output)) ff_output=self.feed_forward(x) x=self.norm3(x+self.dropout(ff_output)) returnx\n\n\n",
    "start": {
      "line": 189,
      "column": 0
    },
    "end": {
      "line": 228,
      "column": 1
    }
  },
  {
    "type": "Class declaration",
    "body": "classTransformer(nn.Module):     \"\"\"\n    The constructor (__init__) initializes the Transformer module. \n    It takes several hyperparameters, including vocabulary sizes for the source and target languages \n    (src_vocab_size and tgt_vocab_size), model dimension (d_model), number of attention heads (num_heads), \n    number of layers (num_layers), dimension of the feedforward network (d_ff), maximum sequence length \n    (max_seq_length), and dropout rate (dropout).\n    It sets up embeddings for both the encoder and decoder (encoder_embedding and decoder_embedding), \n    a positional encoding module (positional_encoding), encoder layers (encoder_layers), \n    decoder layers (decoder_layers), a linear layer (fc), and dropout.\n    \"\"\" def__init__(self,src_vocab_size,tgt_vocab_size,d_model,num_heads,num_layers,d_ff,max_seq_length,dropout):         super(Transformer,self).__init__() self.encoder_embedding=nn.Embedding(src_vocab_size,d_model) self.decoder_embedding=nn.Embedding(tgt_vocab_size,d_model) self.positional_encoding=PositionalEncoding(d_model,max_seq_length) self.encoder_layers=nn.ModuleList([EncoderLayer(d_model,num_heads,d_ff,dropout)for_inrange(num_layers)]) self.decoder_layers=nn.ModuleList([DecoderLayer(d_model,num_heads,d_ff,dropout)for_inrange(num_layers)]) self.fc=nn.Linear(d_model,tgt_vocab_size) self.dropout=nn.Dropout(dropout)  \"\"\"\n     The generate_mask function creates masks for the source and target sequences. \n     It generates a source mask by checking if the source sequence elements are not equal to 0. \n     For the target sequence, it creates a mask by checking if the target sequence elements are not equal \n     to 0 and applies a no-peek mask to prevent attending to future positions.\n    \"\"\" defgenerate_mask(self,src,tgt):         src_mask=(src!=0).unsqueeze(1).unsqueeze(2) tgt_mask=(tgt!=0).unsqueeze(1).unsqueeze(3) seq_length=tgt.size(1) nopeak_mask=(1-torch.triu(torch.ones(1,seq_length,seq_length),diagonal=1)).bool() tgt_mask=tgt_mask&nopeak_mask returnsrc_mask,tgt_mask  \"\"\"\n    The forward function takes source (src) and target (tgt) sequences as input. \n    It generates source and target masks using the generate_mask function. \n    The source and target embeddings are obtained by applying dropout to the positional embeddings of the \n    encoder and decoder embeddings, respectively. \n    The encoder layers are then applied to the source embeddings to get the encoder output (enc_output). \n    The decoder layers are applied to the target embeddings along with the encoder output, source mask, \n    and target mask to get the final decoder output (dec_output). The output is obtained by applying a linear layer to the decoder output.\n    \"\"\" defforward(self,src,tgt):         src_mask,tgt_mask=self.generate_mask(src,tgt) src_embedded=self.dropout(self.positional_encoding(self.encoder_embedding(src))) tgt_embedded=self.dropout(self.positional_encoding(self.decoder_embedding(tgt))) enc_output=src_embedded forenc_layerinself.encoder_layers:             enc_output=enc_layer(enc_output,src_mask)  dec_output=tgt_embedded fordec_layerinself.decoder_layers:             dec_output=dec_layer(dec_output,enc_output,src_mask,tgt_mask)  output=self.fc(dec_output) returnoutput\n\n\n",
    "start": {
      "line": 228,
      "column": 0
    },
    "end": {
      "line": 297,
      "column": 1
    }
  },
  {
    "type": "Assignment",
    "body": "src_vocab_size=5000",
    "start": {
      "line": 297,
      "column": 0
    },
    "end": {
      "line": 297,
      "column": 21
    }
  },
  {
    "type": "Assignment",
    "body": "tgt_vocab_size=5000",
    "start": {
      "line": 298,
      "column": 0
    },
    "end": {
      "line": 298,
      "column": 21
    }
  },
  {
    "type": "Assignment",
    "body": "d_model=512",
    "start": {
      "line": 299,
      "column": 0
    },
    "end": {
      "line": 299,
      "column": 13
    }
  },
  {
    "type": "Assignment",
    "body": "num_heads=8",
    "start": {
      "line": 300,
      "column": 0
    },
    "end": {
      "line": 300,
      "column": 13
    }
  },
  {
    "type": "Assignment",
    "body": "num_layers=6",
    "start": {
      "line": 301,
      "column": 0
    },
    "end": {
      "line": 301,
      "column": 14
    }
  },
  {
    "type": "Assignment",
    "body": "d_ff=2048",
    "start": {
      "line": 302,
      "column": 0
    },
    "end": {
      "line": 302,
      "column": 11
    }
  },
  {
    "type": "Assignment",
    "body": "max_seq_length=100",
    "start": {
      "line": 303,
      "column": 0
    },
    "end": {
      "line": 303,
      "column": 20
    }
  },
  {
    "type": "Assignment",
    "body": "dropout=0.1",
    "start": {
      "line": 304,
      "column": 0
    },
    "end": {
      "line": 304,
      "column": 13
    }
  },
  {
    "type": "Assignment",
    "body": "transformer=Transformer(src_vocab_size,tgt_vocab_size,d_model,num_heads,num_layers,d_ff,max_seq_length,dropout)",
    "start": {
      "line": 306,
      "column": 0
    },
    "end": {
      "line": 306,
      "column": 120
    }
  },
  {
    "type": "Assignment",
    "body": "src_data=torch.randint(1,src_vocab_size,(64,max_seq_length))",
    "start": {
      "line": 309,
      "column": 0
    },
    "end": {
      "line": 309,
      "column": 65
    }
  },
  {
    "type": "Assignment",
    "body": "tgt_data=torch.randint(1,tgt_vocab_size,(64,max_seq_length))",
    "start": {
      "line": 310,
      "column": 0
    },
    "end": {
      "line": 310,
      "column": 65
    }
  },
  {
    "type": "Assignment",
    "body": "criterion=nn.CrossEntropyLoss(ignore_index=0)",
    "start": {
      "line": 318,
      "column": 0
    },
    "end": {
      "line": 318,
      "column": 47
    }
  },
  {
    "type": "Assignment",
    "body": "optimizer=optim.Adam(transformer.parameters(),lr=0.0001,betas=(0.9,0.98),eps=1e-9)",
    "start": {
      "line": 319,
      "column": 0
    },
    "end": {
      "line": 319,
      "column": 88
    }
  },
  {
    "type": "Function call",
    "name": "transformer.train",
    "arguments": "()",
    "start": {
      "line": 321,
      "column": 0
    },
    "end": {
      "line": 321,
      "column": 19
    }
  },
  {
    "type": "For Loop Statement",
    "body": "forepochinrange(100):     optimizer.zero_grad() output=transformer(src_data,tgt_data[:,:-1]) loss=criterion(output.contiguous().view(-1,tgt_vocab_size),tgt_data[:,1:].contiguous().view(-1)) loss.backward() optimizer.step() print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n\n",
    "start": {
      "line": 323,
      "column": 0
    },
    "end": {
      "line": 336,
      "column": 1
    }
  }
]