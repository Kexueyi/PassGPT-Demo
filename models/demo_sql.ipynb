{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database + GPT\n",
    "\n",
    "Techniqually its not easy to make GPT fully intellegently utilze the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In week 3, we covered major topics primarily focused on text analysis and dimensionality reduction methods used in data processing and machine learning. Here’s a summary of the main concepts along with their related sub-concepts:\n",
      "\n",
      "1. **Term Weighting Schemes**\n",
      "   - **TF-IDF**: Term Frequency-Inverse Document Frequency, a statistical measure used to evaluate the importance of a word to a document in a collection or corpus.\n",
      "   - **BM25**: A ranking function used by search engines to estimate the relevance of documents to a given search query.\n",
      "\n",
      "2. **Topic Modeling**\n",
      "   - **LDA** (Latent Dirichlet Allocation): A generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.\n",
      "   - **LSA** (Latent Semantic Analysis): A technique in natural language processing of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.\n",
      "   - **pLSA** (Probabilistic Latent Semantic Analysis): A statistical technique for analyzing two-mode and co-occurrence data.\n",
      "   - **NMF** (Non-negative Matrix Factorization): A group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements.\n",
      "\n",
      "3. **Dimensionality Reduction**\n",
      "   - **PCA** (Principal Component Analysis): A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
      "   - **SVD** (Singular Value Decomposition): A factorization of a real or complex matrix that generalizes the eigendecomposition of a square normal matrix to any \\(m \\times n\\) matrix via an extension of the polar decomposition.\n",
      "\n",
      "These concepts and their related sub-concepts are foundational to understanding advanced methods in data science, particularly in the realms of text mining, search engine technologies, and data simplification for easier analysis and visualization.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pymysql\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API\"))\n",
    "\n",
    "def get_db_connection():\n",
    "    return pymysql.connect(\n",
    "        host='pass-gpt-db.c3oum0k4qlkb.ap-southeast-1.rds.amazonaws.com',\n",
    "        port=3306,\n",
    "        user='root',\n",
    "        password=os.getenv(\"AWS_IRD_ROOT_KEY\"),\n",
    "        database='passgpt',\n",
    "        charset='utf8mb4',\n",
    "        cursorclass=pymysql.cursors.DictCursor\n",
    "    )\n",
    "\n",
    "def clean_sql_query(sql_query):\n",
    "    # 清理 SQL 语句，移除多余的字符\n",
    "    sql_query = sql_query.replace(\"`\", \"\").replace(\"sql\\n\", \"\").replace(\"\\n\", \" \")\n",
    "    return ' '.join(sql_query.split())\n",
    "\n",
    "def sql_query_gpt(user_input):\n",
    "    connection = get_db_connection()\n",
    "    \n",
    "    # 提供数据库结构信息作为系统提示\n",
    "    database_description = \"\"\"\n",
    "    Database 'passgpt' consists of several tables: 'courses', 'files', and 'concepts'.\n",
    "    - The 'courses' table has columns: course_id (primary key), course_code, course_name, course_description.\n",
    "    - The 'files' table relates to 'courses' via course_id and includes: file_id (primary key), file_name, title, file_type, file_path, teaching_week, creation_date.\n",
    "    - The 'concepts' table includes concepts that relate to files and can be hierarchical: concept_id (primary key), parent_id, file_id, concept_name, concept_page, concept_description.\n",
    "    Relations:\n",
    "    - 'files' to 'courses' via course_id.\n",
    "    - 'concepts' to 'files' via file_id and 'concepts' to other 'concepts' via parent_id.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 初始化系统与用户的对话，包括数据库结构信息\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": database_description},\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI trained to interpret user's input and generate SQL queries based on user's natural language descriptions given the database schema. Please only generate clean SQL queries without explanations.\"},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # 使用OpenAI的ChatGPT模型处理对话\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  \n",
    "            messages=messages\n",
    "        )\n",
    "        if chat_completion.choices and len(chat_completion.choices) > 0:\n",
    "            sql_query_cmd = clean_sql_query(chat_completion.choices[0].message.content.strip())\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(sql_query_cmd)\n",
    "                sql_query_results = cursor.fetchall()\n",
    "            connection.close()\n",
    "            return sql_query_cmd, sql_query_results# explanation, sql_query \n",
    "        else:\n",
    "            connection.close()\n",
    "            return \"No response was generated by GPT.\"\n",
    "    except Exception as e:\n",
    "        connection.close()\n",
    "        return f\"Error during the chat completion or SQL execution: {e}\"\n",
    "\n",
    "def course_query_gpt(query, course_info):\n",
    "    \"\"\"\n",
    "    Submits a query along with the formatted course information to GPT for processing,\n",
    "    using the newer client.chat.completions.create interface.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{course_info} Your task is to provide a comprehensive response based on the information provided.\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  \n",
    "            messages=messages,\n",
    "        )\n",
    "        if chat_completion.choices and len(chat_completion.choices) > 0:\n",
    "            last_message = chat_completion.choices[0].message.content.strip()\n",
    "            return last_message\n",
    "        else:\n",
    "            return \"No response was returned by GPT.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error querying GPT with course info: {e}\"\n",
    "     \n",
    "# Example use\n",
    "user_input = \"what we have learnt in week 3, any main concepts, and its related sub concepts?\"\n",
    "query_cmd, query_results = sql_query_gpt(user_input)\n",
    "output = course_query_gpt(user_input, query_results)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why don't we integrate them into a single model?\n",
    "### Advantages:\n",
    "- Less OPENAI api call cost\n",
    "- Faster response time\n",
    "- ...\n",
    "### Challenges:\n",
    "- More trial-and-error prompt tuning\n",
    "- Current OPAI API limit 4096 tokens\n",
    "- API call can't excute sql query by himself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymysql\n",
    "from openai import OpenAI\n",
    "\n",
    "class PassGPT:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.database_description = \"\"\"\n",
    "        Database 'passgpt' consists of several tables: 'courses', 'files', and 'concepts'.\n",
    "        - The 'courses' table has columns: course_id (primary key), course_code, course_name, course_description.\n",
    "        - The 'files' table relates to 'courses' via course_id and includes: file_id (primary key), file_name, title, file_type, file_path, teaching_week, creation_date.\n",
    "        - The 'concepts' table includes concepts that relate to files and can be hierarchical: concept_id (primary key), parent_id(NULL for top concepts), file_id, concept_name, concept_page, concept_description.\n",
    "        Relations:\n",
    "        - 'files' to 'courses' via course_id.\n",
    "        - 'concepts' to 'files' via file_id and 'concepts' to other 'concepts' via parent_id(hierarchical).\n",
    "        \"\"\"\n",
    "    \n",
    "    def get_db_connection(self):\n",
    "        return pymysql.connect(\n",
    "            host='pass-gpt-db.c3oum0k4qlkb.ap-southeast-1.rds.amazonaws.com',\n",
    "            port=3306,\n",
    "            user='root',\n",
    "            password=os.getenv(\"AWS_IRD_ROOT_KEY\"), # Replace with your own AWS RDS root password\n",
    "            database='passgpt',\n",
    "            charset='utf8mb4',\n",
    "            cursorclass=pymysql.cursors.DictCursor\n",
    "        )\n",
    "    \n",
    "    def clean_sql_query(self, sql_query):\n",
    "        sql_query = sql_query.replace(\"`\", \"\").replace(\"sql\\n\", \"\").replace(\"\\n\", \" \")\n",
    "        return ' '.join(sql_query.split())\n",
    "\n",
    "    def execute_sql_query(self, sql_query):\n",
    "        connection = self.get_db_connection()\n",
    "        try:\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(sql_query)\n",
    "                return cursor.fetchall()\n",
    "        finally:\n",
    "            connection.close()\n",
    "\n",
    "    def generate_and_process_query(self, user_input):\n",
    "        prompt = f\"{self.database_description} Based on the database schema, generate SQL queris to retrieve all relevant data with detailed information, based on the user input: '{user_input}'. \n",
    "            Then, your need to provide a comprehensive response based on the information provided, describe the results in a detailed, human-readable form for education need.\n",
    "            You may need to provide a detailed explanation of the results, including the main concepts and related sub-concepts.\n",
    "            If you are asked to provide quiz/question/test, for code questions, the questions should be single choice questions testing understanding of the code representing related concepts, you should also provide the code segment;for not specified code questions, the questions should have multiple choice questions testing understanding of the terms. Finally, you should give the answer\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        try:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                model=\"gpt-4-turbo\",  \n",
    "                messages=messages\n",
    "            )\n",
    "            if chat_completion.choices and len(chat_completion.choices) > 0:\n",
    "                #TODO: this is where we can make it more integrate into GPT\n",
    "                sql_query = self.clean_sql_query(chat_completion.choices[0].message.content.strip())\n",
    "                results = self.execute_sql_query(sql_query)\n",
    "                return results\n",
    "            else:\n",
    "                return \"No response was generated by GPT.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error during the chat completion or SQL execution: {e}\"\n",
    "\n",
    "# Example usage:\n",
    "api_key = os.getenv(\"OPENAI_API\")\n",
    "db_query_processor = PassGPT(api_key=api_key)\n",
    "user_input = \"what we have learnt in week 3, any main concepts, in which page?\"\n",
    "response = db_query_processor.generate_and_process_query(user_input)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
