{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_course_info(file_path):\n",
    "    \"\"\"\n",
    "    Loads the course information from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        course_info = json.load(file)\n",
    "    return course_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for traditional NLP, please modify this function for your json data structure\n",
    "def format_course_info_for_gpt(course_info):\n",
    "    \"\"\"\n",
    "    Formats the course information into a text summary that GPT can understand.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for course, details in course_info.items():\n",
    "        concepts = ', '.join([concept['name'] for concept in details['Concepts']])\n",
    "        key_functions = ', '.join(details['code']['Key_functions'])\n",
    "        summary = f\"{details['Teaching Week']}: {details['Title']} covers concepts {concepts} and key functions {key_functions}.\"\n",
    "        summaries.append(summary)\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"info.json\"\n",
    "course_info = load_course_info(file_path)\n",
    "course_info_summary = format_course_info_for_gpt(course_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"week 1: Introduction to Natural Language Processing covers concepts Overview of NLP, Historical Background, Approaches to NLP, Preprocessing Techniques and key functions re.sub(pattern, replacement, string), PorterStemmer(), word_tokenize(), nltk.pos_tag(), nltk.word_tokenize(), set(stopwords.words('english')). week 2: Linguistic Analysis and Information Extraction covers concepts Introduction to Information Extraction, Named Entity Recognition (NER), Part-Of-Speech Tagging, Dependency Parsing and key functions NER(), spacy.load(), nlp.pipe(), spacy.load(), nlp(). week 3: Term Weighting Scheme and Topic Modelling covers concepts Term Weighting Schemes, Topic Modeling, Dimensionality Reduction and key functions TfidfVectorizer(), BM25Okapi(), gensim.models.LdaMulticore(), LsiModel(), NMF(), PCA(), TruncatedSVD(). week 4: Traditional Machine Learning Methods and NLP Applications covers concepts Text Classification, Clustering and key functions accuracy_score, f1_score, confusion_matrix, svm.SVC, ELMClassifier, GaussianProcessClassifier, KMeans, KMeans.fit, TruncatedSVD, make_pipeline, Normalizer. week 5: Evaluation Metrics and Word Embeddings covers concepts Evaluation Metrics, Word Embeddings and key functions confusion_matrix, accuracy_score, precision_score, recall_score, sns.heatmap, LogisticRegression, log_regression.fit, metrics.roc_curve, metrics.roc_auc_score, nltk.word_tokenize, nltk.translate.bleu_score.SmoothingFunction().method1, nltk.translate.bleu_score.sentence_bleu, nltk.translate.meteor, evaluate.load('rouge'), Word2Vec, model.save, Word2Vec.load, model.wv, model.wv.similarity, glove_model.most_similar. week 6: Neural Language Models covers concepts Sequential Data, RNNs, LSTMs, GRUs, Bi-Directional RNNs and key functions tf.keras.layers.experimental.preprocessing.TextVectorization, encoder.adapt, encoder.get_vocabulary, tf.keras.Sequential, encoder, tf.keras.layers.Embedding, tf.keras.layers.SimpleRNN, tf.keras.layers.Dense, simpleRNN.compile, tf.keras.losses.BinaryCrossentropy, tf.keras.optimizers.Adam, simpleRNN.fit, tf.keras.layers.LSTM, tf.keras.layers.Bidirectional, tf.keras.layers.GRU. week 7: Transformers covers concepts Seq2Seq Models, Attention Mechanism, Transformer Models and key functions torch.matmul, torch.softmax, view, transpose, contiguous, masked_fill, nn.Linear, nn.ReLU, torch.zeros, torch.arange, torch.exp, torch.sin, torch.cos, torch.unsqueeze, self.register_buffer, MultiHeadAttention, PositionWiseFeedForward, nn.LayerNorm, nn.Dropout, nn.Embedding, PositionalEncoding, nn.ModuleList, EncoderLayer, DecoderLayer, torch.unsqueeze, torch.triu, torch.ones, torch.randint, nn.CrossEntropyLoss, optim.Adam, transformer.train, optimizer.zero_grad, transformer, criterion, loss.backward, optimizer.step. week 8: HyperParameter Tuning covers concepts K-Fold Cross Validation, Optimisers, Loss Functions, Batch Size and Learning Rate, Epochs and Early Stopping, Gradient Clipping, Regularisation Techniques and key functions pd.read_csv, re.sub, train_test_split, TextVectorization, tf.keras.Sequential, EarlyStopping, compile and fit, accuracy_score, StratifiedKFold. week 9: Transformer Based Large Language Models covers concepts Transformer Recap, Pre Training Objectives, Autoregressive Models, Autoencoding Models, Positional Encodings, Tokenizers, Pre-training Process, Fine-tuning Process, BERT Variants, GPT Series, T5 and BART Models, XLNet and key functions load_dataset, AutoTokenizer.from_pretrained, AutoModelForSequenceClassification.from_pretrained, TrainingArguments, Trainer, compute_metrics. week 10: A Survey of NLP Applications Across Diverse Industries covers concepts Overview, Healthcare, Finance and Banking, Retail and E-Commerce, Legal Industry, Automotive Industry, Publishing Industry, Education, Travel and Hospitality, Media and Entertainment, Government & Public Sector and key functions IMDB (from torchtext.datasets), precision_score, recall_score, f1_score (from sklearn.metrics), compute_embeddings, extract_positive_sentences, summarize_commonalities, suggest_movie_plot. week 11: Deep-dive into NLP covers concepts Fraud Detection in E-commerce, Human Trafficking Detection, ChatGPT-based Applications and key functions re.sub, str.lower, TfidfVectorizer, train_test_split, SVC, accuracy_score.\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_info_summary #展示GPT需要了解的知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt_with_code_info(query, code_info):\n",
    "    \"\"\"\n",
    "    Submits a query along with the formatted course information to GPT for processing,\n",
    "    using the newer client.chat.completions.create interface.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{code_info} Your task is to provide a comprehensive response based on the code and concepts provided. \"\n",
    "        \"if you are asked to provide code questions, the questions shoulde single choice questions testing understanding of the code and concept. Before the questions, you should provide the code segment\"\n",
    "        \"if you are asked to provide questions (not specified code questions), the questions should have multiple choice questions testing understanding of the terms.\"\n",
    "        \"Finally, you should give the answer\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  # Adjust the model as needed\n",
    "            messages=messages,\n",
    "        )\n",
    "        # Assuming we need to get the last message from the chat completion properly\n",
    "        # Adjusting for the correct way to access the last message's content\n",
    "        if chat_completion.choices and len(chat_completion.choices) > 0:\n",
    "            last_message = chat_completion.choices[0].message.content.strip()\n",
    "            return last_message\n",
    "        else:\n",
    "            return \"No response was returned by GPT.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error querying GPT with course info: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Code Segment Presentations and Questions from Week 5 - \"Evaluation Metrics and Word Embeddings\"\n",
      "\n",
      "**Code Segment 1:**\n",
      "```python\n",
      "metrics.roc_auc_score(y_true, y_scores)\n",
      "```\n",
      "**Question 1:**\n",
      "What does the `roc_auc_score` function compute in the context of evaluation metrics?\n",
      "- A) The relation between True Negative Rate and False Positive Rate.\n",
      "- B) The area under the Receiver Operating Characteristic curve.\n",
      "- C) The maximum score for precision.\n",
      "- D) The total number of correct predictions.\n",
      "\n",
      "**Answer:** B) The area under the Receiver Operating Characteristic curve.\n",
      "\n",
      "---\n",
      "\n",
      "**Code Segment 2:**\n",
      "```python\n",
      "nltk.translate.bleu_score.sentence_bleu(reference, candidate, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
      "```\n",
      "**Question 2:**\n",
      "What is the purpose of the `sentence_bleu` function from the `nltk` library?\n",
      "- A) It predicts the next word in a sentence based on context.\n",
      "- B) It calculates the BLEU score to assess the quality of machine-translated text against a reference translation.\n",
      "- C) It sums up the total sentiment score of a candidate sentence.\n",
      "- D) It tokenizes the input sentence into subwords.\n",
      "\n",
      "**Answer:** B) It calculates the BLEU score to assess the quality of machine-translated text against a reference translation.\n",
      "\n",
      "---\n",
      "\n",
      "**Code Segment 3:**\n",
      "```python\n",
      "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\n",
      "```\n",
      "**Question 3:**\n",
      "What is visualized using the seaborn `heatmap` function for the confusion matrix?\n",
      "- A) The frequency of different word embeddings.\n",
      "- B) A visual representation showing the accuracy of predictions in a classification model.\n",
      "- C) A plot showing the loss over epochs in model training.\n",
      "- D) Graph showing the similarity between different word vectors.\n",
      "\n",
      "**Answer:** B) A visual representation showing the accuracy of predictions in a classification model.\n",
      "\n",
      "Each of these questions explores understanding of the code used in Week 5 related to evaluation metrics and their interpretations within machine learning models.\n"
     ]
    }
   ],
   "source": [
    "query = \"Please provide some code questions for Week 5.\"\n",
    "response = query_gpt_with_code_info(query, course_info_summary)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
